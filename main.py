import gym
from cc_env import CrissCross
import numpy as np
import criss_cross


env = CrissCross(load = 0.5) # create the criss-cross network environment with load 0.5
n_episodes = 20000 # number of episodes to run
av_reward = np.zeros(n_episodes)
av_r = 0.
for i in range(n_episodes):
    k = 0
    done = False
    env.reset() # reset the environment
    av_r = 0.
    while not done: # while the system is not empty
        action = env.action_space.sample() # sample a random action
        obs, reward, done, info = env.step(action) # make a step
        av_r = reward +  av_r # accumulate the cost
        k = k + 1

    av_reward[i]=av_r # save total holding cost generated by the ith episode

print(-np.mean(av_reward),'+/-',1.96*np.std(av_reward)/np.sqrt(n_episodes))



from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy


env = make_vec_env('criss_cross-v0', n_envs=4)


model = PPO(MlpPolicy, env, verbose=1, n_steps=2048, gamma = 1)
model.learn(total_timesteps=1000000)

model.save("cc_1")
env = gym.make('criss_cross-v0')
n_episodes = 20000
res_mean, res_std = evaluate_policy(model, env, n_eval_episodes=n_episodes)
print(-res_mean,'+/-',1.96*res_std/np.sqrt(n_episodes))
